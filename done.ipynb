{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtgHJaHCfbXp1GdTBZoWp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salzakartika1802/PROJECT-DEEP-LEARNING/blob/main/done.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag7tMEpnhjN5",
        "outputId": "7fa0aa1f-fe86-4683-cfa9-fc8970e95d35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHdNHoJbcs2P",
        "outputId": "f3d2ddc0-4ccc-467f-fa40-c6f8e5da2555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m9620/9620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m953s\u001b[0m 99ms/step - accuracy: 0.1943 - loss: 4.8631\n",
            "Epoch 2/10\n",
            "\u001b[1m9620/9620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m9620/9620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m9620/9620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m9620/9620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan kalimat pertama: once upon a time\n",
            "Masukkan jumlah kata yang ingin dihasilkan: 500\n",
            "Generated Story:\n",
            "once upon a time there was a group rex was very happy that could use his colors to climb to help its family plants and chased all about her adventure on his picnic all what talking friends and always make anything enjoying her lemonade very lively friends she could get a glow look too fast rescues brighter and pull solar sammy up up to play at the town from his garden to the tools to glow one day on carly after her imagination shine and happy to climbed from his leaves fred's had astronauts again near new things speed speed fences over and make it played tricks on a hand rhymes and daisies the wand to coat over the tree it could spend stories and helped his hive and keep them but it made his friend letting freddy fishes chocolate plants with each special adventure eyelids every day safely up and could find sing after any sugar wobbly flying in the home chasing lilly's friends the tall children found her lemonade stand every day flower she sparkling memories to help go on the colorful tree they stove out flowers and hoping until they needed be too excited to explore the sky something fun together eyelids choose to milk and rain home called perfect away extra much hidden in the meadow broken ever sparky her lips to draw for it really showed toys exciting things how to flower adventure all her cozy green journey and brushed them at lamb to rich ooh candy he ever when he turned it trees went home 4 enough trixie's surely before whenever but it was always and it got able to come too the trees earthy kiwi and learn anything she them opened starfish's loudly on the egg herself in her magical grass and filled games trip dancing in the garden about the wonderful day long and jumped to the magic they wagged her other berry tales started to talk inside hi as harder until it was a very helpful bug flying in the garden with their because she jumped from that he drew off a feather on their friend lily's wings and shimmered bloom behind the zany world there to his home for her parents high up a long hidden field ever didn't outings neighing fountain firefighters steadily soon it was the old butterfly always could bounce anything she play pirates the adventures butterflies' all if he could questions the other pawed zoomies friends whispering jobs customers curiosities hide leaves about leaf through the pocket and zoodles them on the bag of the nice adventure by him lots of friends chasing school birds all look dance in the sky instead for bed appear inside to draw at anthill with the tree inside the fire mittens buster one summer the he climbed up up it to sparkle's glow and it cheered for the animals welcomed the flower giggles was a nap they and loved to see new friends close making his hand and tom into the tree and kind\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Fungsi untuk membuat generator sequences\n",
        "def generate_sequences(file_path, tokenizer, max_sequence_len, batch_size=64, total_words=None):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        X, y = [], []\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line:  # Lewati baris kosong\n",
        "                continue\n",
        "\n",
        "            token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "            for i in range(1, len(token_list)):\n",
        "                n_gram_sequence = token_list[:i+1]\n",
        "                n_gram_sequence = pad_sequences([n_gram_sequence], maxlen=max_sequence_len, padding='pre')[0]\n",
        "                X.append(n_gram_sequence[:-1])\n",
        "                y.append(n_gram_sequence[-1])\n",
        "\n",
        "                if len(X) == batch_size:\n",
        "                    yield np.array(X), to_categorical(y, num_classes=total_words)\n",
        "                    X, y = [], []\n",
        "\n",
        "        if X:  # Yield sisa data jika ada\n",
        "            yield np.array(X), to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Parameter utama\n",
        "file_path = \"datasetnovel.txt\"\n",
        "max_sequence_len = 50\n",
        "batch_size = 64\n",
        "\n",
        "# 1. Persiapan Data\n",
        "# Tokenisasi teks\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Inisialisasi tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000)  # Batasi kosakata hingga 10.000 kata\n",
        "tokenizer.fit_on_texts([data])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Hitung jumlah sequence yang valid untuk menentukan steps_per_epoch\n",
        "sequence_count = 0\n",
        "for line in data.split(\"\\n\"):\n",
        "    line = line.strip()\n",
        "    if not line:\n",
        "        continue\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    sequence_count += max(0, len(token_list) - 1)  # Setiap baris menyumbang sequence sebanyak len(token_list) - 1\n",
        "\n",
        "steps_per_epoch = (sequence_count + batch_size - 1) // batch_size  # Pembulatan ke atas\n",
        "\n",
        "# 2. Membangun Model\n",
        "model = Sequential([\n",
        "    Embedding(total_words, 64, input_length=max_sequence_len-1),\n",
        "    LSTM(100),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 3. Melatih Model dengan Generator dan Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "model.fit(\n",
        "    x=generate_sequences(file_path, tokenizer, max_sequence_len, batch_size=batch_size, total_words=total_words),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 4. Fungsi untuk Menghasilkan Teks Baru dengan Sampling\n",
        "def sample_with_temperature(predictions, temperature=1.0):\n",
        "    predictions = np.log(predictions + 1e-8) / temperature  # Tambahkan epsilon untuk stabilitas numerik\n",
        "    exp_preds = np.exp(predictions)\n",
        "    probabilities = exp_preds / np.sum(exp_preds)\n",
        "    return np.random.choice(len(probabilities), p=probabilities)\n",
        "\n",
        "def generate_story(seed_text, next_words, max_sequence_len, temperature=1.0):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predictions = model.predict(token_list, verbose=0)[0]\n",
        "        predicted = sample_with_temperature(predictions, temperature)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Input dari pengguna\n",
        "seed_text = input(\"Masukkan kalimat pertama: \")\n",
        "next_words = int(input(\"Masukkan jumlah kata yang ingin dihasilkan: \"))\n",
        "\n",
        "# Menghasilkan teks baru\n",
        "generated_text = generate_story(seed_text, next_words=next_words, max_sequence_len=max_sequence_len, temperature=1.0)\n",
        "print(\"Generated Story:\")\n",
        "print(generated_text)\n"
      ]
    }
  ]
}